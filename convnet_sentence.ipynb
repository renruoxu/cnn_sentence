{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    }
   ],
   "source": [
    "import cPickle\n",
    "import numpy as np\n",
    "from collections import defaultdict, OrderedDict\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import re\n",
    "import warnings\n",
    "import sys\n",
    "warnings.filterwarnings(\"ignore\")   \n",
    "\n",
    "#different non-linearities\n",
    "def ReLU(x):\n",
    "    y = T.maximum(0.0, x)\n",
    "    return(y)\n",
    "def Sigmoid(x):\n",
    "    y = T.nnet.sigmoid(x)\n",
    "    return(y)\n",
    "def Tanh(x):\n",
    "    y = T.tanh(x)\n",
    "    return(y)\n",
    "def Iden(x):\n",
    "    y = x\n",
    "    return(y)\n",
    "       \n",
    "def train_conv_net(datasets,\n",
    "                   U,\n",
    "                   img_w=300, \n",
    "                   filter_hs=[3,4,5],\n",
    "                   hidden_units=[100,2], \n",
    "                   dropout_rate=[0.5],\n",
    "                   shuffle_batch=True,\n",
    "                   n_epochs=25, \n",
    "                   batch_size=50, \n",
    "                   lr_decay = 0.95,\n",
    "                   conv_non_linear=\"relu\",\n",
    "                   activations=[Iden],\n",
    "                   sqr_norm_lim=9,\n",
    "                   non_static=True):\n",
    "    \"\"\"\n",
    "    Train a simple conv net\n",
    "    img_h = sentence length (padded where necessary)\n",
    "    img_w = word vector length (300 for word2vec)\n",
    "    filter_hs = filter window sizes    \n",
    "    hidden_units = [x,y] x is the number of feature maps (per filter window), and y is the penultimate layer\n",
    "    sqr_norm_lim = s^2 in the paper\n",
    "    lr_decay = adadelta decay parameter\n",
    "    \"\"\"    \n",
    "    rng = np.random.RandomState(3435)\n",
    "    img_h = len(datasets[0][0])-1  \n",
    "    filter_w = img_w    \n",
    "    feature_maps = hidden_units[0]\n",
    "    filter_shapes = []\n",
    "    pool_sizes = []\n",
    "    for filter_h in filter_hs:\n",
    "        filter_shapes.append((feature_maps, 1, filter_h, filter_w))\n",
    "        pool_sizes.append((img_h-filter_h+1, img_w-filter_w+1))\n",
    "    parameters = [(\"image shape\",img_h,img_w),(\"filter shape\",filter_shapes), (\"hidden_units\",hidden_units),\n",
    "                  (\"dropout\", dropout_rate), (\"batch_size\",batch_size),(\"non_static\", non_static),\n",
    "                    (\"learn_decay\",lr_decay), (\"conv_non_linear\", conv_non_linear), (\"non_static\", non_static)\n",
    "                    ,(\"sqr_norm_lim\",sqr_norm_lim),(\"shuffle_batch\",shuffle_batch)]\n",
    "    print parameters    \n",
    "    \n",
    "    #define model architecture\n",
    "    index = T.lscalar()\n",
    "    x = T.matrix('x')   \n",
    "    y = T.ivector('y')\n",
    "    Words = theano.shared(value = U, name = \"Words\")\n",
    "    zero_vec_tensor = T.vector()\n",
    "    zero_vec = np.zeros(img_w)\n",
    "    set_zero = theano.function([zero_vec_tensor], updates=[(Words, T.set_subtensor(Words[0,:], zero_vec_tensor))])\n",
    "    layer0_input = Words[T.cast(x.flatten(),dtype=\"int32\")].reshape((x.shape[0],1,x.shape[1],Words.shape[1]))                                  \n",
    "    conv_layers = []\n",
    "    layer1_inputs = []\n",
    "    for i in xrange(len(filter_hs)):\n",
    "        filter_shape = filter_shapes[i]\n",
    "        pool_size = pool_sizes[i]\n",
    "        conv_layer = LeNetConvPoolLayer(rng, input=layer0_input,image_shape=(batch_size, 1, img_h, img_w),\n",
    "                                filter_shape=filter_shape, poolsize=pool_size, non_linear=conv_non_linear)\n",
    "        layer1_input = conv_layer.output.flatten(2)\n",
    "        conv_layers.append(conv_layer)\n",
    "        layer1_inputs.append(layer1_input)\n",
    "    layer1_input = T.concatenate(layer1_inputs,1)\n",
    "    hidden_units[0] = feature_maps*len(filter_hs)    \n",
    "    classifier = MLPDropout(rng, input=layer1_input, layer_sizes=hidden_units, activations=activations, dropout_rates=dropout_rate)\n",
    "    \n",
    "    #define parameters of the model and update functions using adadelta\n",
    "    params = classifier.params     \n",
    "    for conv_layer in conv_layers:\n",
    "        params += conv_layer.params\n",
    "    if non_static:\n",
    "        #if word vectors are allowed to change, add them as model parameters\n",
    "        params += [Words]\n",
    "    cost = classifier.negative_log_likelihood(y) \n",
    "    dropout_cost = classifier.dropout_negative_log_likelihood(y)           \n",
    "    grad_updates = sgd_updates_adadelta(params, dropout_cost, lr_decay, 1e-6, sqr_norm_lim)\n",
    "    \n",
    "    #shuffle dataset and assign to mini batches. if dataset size is not a multiple of mini batches, replicate \n",
    "    #extra data (at random)\n",
    "    np.random.seed(3435)\n",
    "    if datasets[0].shape[0] % batch_size > 0:\n",
    "        extra_data_num = batch_size - datasets[0].shape[0] % batch_size\n",
    "        train_set = np.random.permutation(datasets[0])   \n",
    "        extra_data = train_set[:extra_data_num]\n",
    "        new_data=np.append(datasets[0],extra_data,axis=0)\n",
    "    else:\n",
    "        new_data = datasets[0]\n",
    "    new_data = np.random.permutation(new_data)\n",
    "    n_batches = new_data.shape[0]/batch_size\n",
    "    n_train_batches = int(np.round(n_batches*0.9))\n",
    "    #divide train set into train/val sets \n",
    "    test_set_x = datasets[1][:,:img_h] \n",
    "    test_set_y = np.asarray(datasets[1][:,-1],\"int32\")\n",
    "    train_set = new_data[:n_train_batches*batch_size,:]\n",
    "    val_set = new_data[n_train_batches*batch_size:,:]     \n",
    "    train_set_x, train_set_y = shared_dataset((train_set[:,:img_h],train_set[:,-1]))\n",
    "    val_set_x, val_set_y = shared_dataset((val_set[:,:img_h],val_set[:,-1]))\n",
    "    n_val_batches = n_batches - n_train_batches\n",
    "    val_model = theano.function([index], classifier.errors(y),\n",
    "         givens={\n",
    "            x: val_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "            y: val_set_y[index * batch_size: (index + 1) * batch_size]})\n",
    "            \n",
    "    #compile theano functions to get train/val/test errors\n",
    "    test_model = theano.function([index], classifier.errors(y),\n",
    "             givens={\n",
    "                x: train_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "                y: train_set_y[index * batch_size: (index + 1) * batch_size]})               \n",
    "    train_model = theano.function([index], cost, updates=grad_updates,\n",
    "          givens={\n",
    "            x: train_set_x[index*batch_size:(index+1)*batch_size],\n",
    "            y: train_set_y[index*batch_size:(index+1)*batch_size]})     \n",
    "    test_pred_layers = []\n",
    "    test_size = test_set_x.shape[0]\n",
    "    test_layer0_input = Words[T.cast(x.flatten(),dtype=\"int32\")].reshape((test_size,1,img_h,Words.shape[1]))\n",
    "    for conv_layer in conv_layers:\n",
    "        test_layer0_output = conv_layer.predict(test_layer0_input, test_size)\n",
    "        test_pred_layers.append(test_layer0_output.flatten(2))\n",
    "    test_layer1_input = T.concatenate(test_pred_layers, 1)\n",
    "    test_y_pred = classifier.predict(test_layer1_input)\n",
    "    test_error = T.mean(T.neq(test_y_pred, y))\n",
    "    test_model_all = theano.function([x,y], test_error)   \n",
    "    \n",
    "    #start training over mini-batches\n",
    "    print '... training'\n",
    "    epoch = 0\n",
    "    best_val_perf = 0\n",
    "    val_perf = 0\n",
    "    test_perf = 0       \n",
    "    cost_epoch = 0    \n",
    "    while (epoch < n_epochs):        \n",
    "        epoch = epoch + 1\n",
    "        if shuffle_batch:\n",
    "            for minibatch_index in np.random.permutation(range(n_train_batches)):\n",
    "                cost_epoch = train_model(minibatch_index)\n",
    "                set_zero(zero_vec)\n",
    "        else:\n",
    "            for minibatch_index in xrange(n_train_batches):\n",
    "                cost_epoch = train_model(minibatch_index)  \n",
    "                set_zero(zero_vec)\n",
    "        train_losses = [test_model(i) for i in xrange(n_train_batches)]\n",
    "        train_perf = 1 - np.mean(train_losses)\n",
    "        val_losses = [val_model(i) for i in xrange(n_val_batches)]\n",
    "        val_perf = 1- np.mean(val_losses)                        \n",
    "        print('epoch %i, train perf %f %%, val perf %f' % (epoch, train_perf * 100., val_perf*100.))\n",
    "        if val_perf >= best_val_perf:\n",
    "            best_val_perf = val_perf\n",
    "            test_loss = test_model_all(test_set_x,test_set_y)        \n",
    "            test_perf = 1- test_loss         \n",
    "    return test_perf\n",
    "\n",
    "def shared_dataset(data_xy, borrow=True):\n",
    "        \"\"\" Function that loads the dataset into shared variables\n",
    "\n",
    "        The reason we store our dataset in shared variables is to allow\n",
    "        Theano to copy it into the GPU memory (when code is run on GPU).\n",
    "        Since copying data into the GPU is slow, copying a minibatch everytime\n",
    "        is needed (the default behaviour if the data is not in a shared\n",
    "        variable) would lead to a large decrease in performance.\n",
    "        \"\"\"\n",
    "        data_x, data_y = data_xy\n",
    "        shared_x = theano.shared(np.asarray(data_x,\n",
    "                                               dtype=theano.config.floatX),\n",
    "                                 borrow=borrow)\n",
    "        shared_y = theano.shared(np.asarray(data_y,\n",
    "                                               dtype=theano.config.floatX),\n",
    "                                 borrow=borrow)\n",
    "        return shared_x, T.cast(shared_y, 'int32')\n",
    "        \n",
    "def sgd_updates_adadelta(params,cost,rho=0.95,epsilon=1e-6,norm_lim=9,word_vec_name='Words'):\n",
    "    \"\"\"\n",
    "    adadelta update rule, mostly from\n",
    "    https://groups.google.com/forum/#!topic/pylearn-dev/3QbKtCumAW4 (for Adadelta)\n",
    "    \"\"\"\n",
    "    updates = OrderedDict({})\n",
    "    exp_sqr_grads = OrderedDict({})\n",
    "    exp_sqr_ups = OrderedDict({})\n",
    "    gparams = []\n",
    "    for param in params:\n",
    "        empty = np.zeros_like(param.get_value())\n",
    "        exp_sqr_grads[param] = theano.shared(value=as_floatX(empty),name=\"exp_grad_%s\" % param.name)\n",
    "        gp = T.grad(cost, param)\n",
    "        exp_sqr_ups[param] = theano.shared(value=as_floatX(empty), name=\"exp_grad_%s\" % param.name)\n",
    "        gparams.append(gp)\n",
    "    for param, gp in zip(params, gparams):\n",
    "        exp_sg = exp_sqr_grads[param]\n",
    "        exp_su = exp_sqr_ups[param]\n",
    "        up_exp_sg = rho * exp_sg + (1 - rho) * T.sqr(gp)\n",
    "        updates[exp_sg] = up_exp_sg\n",
    "        step =  -(T.sqrt(exp_su + epsilon) / T.sqrt(up_exp_sg + epsilon)) * gp\n",
    "        updates[exp_su] = rho * exp_su + (1 - rho) * T.sqr(step)\n",
    "        stepped_param = param + step\n",
    "        if (param.get_value(borrow=True).ndim == 2) and (param.name!='Words'):\n",
    "            col_norms = T.sqrt(T.sum(T.sqr(stepped_param), axis=0))\n",
    "            desired_norms = T.clip(col_norms, 0, T.sqrt(norm_lim))\n",
    "            scale = desired_norms / (1e-7 + col_norms)\n",
    "            updates[param] = stepped_param * scale\n",
    "        else:\n",
    "            updates[param] = stepped_param      \n",
    "    return updates \n",
    "\n",
    "def as_floatX(variable):\n",
    "    if isinstance(variable, float):\n",
    "        return np.cast[theano.config.floatX](variable)\n",
    "\n",
    "    if isinstance(variable, np.ndarray):\n",
    "        return np.cast[theano.config.floatX](variable)\n",
    "    return theano.tensor.cast(variable, theano.config.floatX)\n",
    "    \n",
    "def safe_update(dict_to, dict_from):\n",
    "    \"\"\"\n",
    "    re-make update dictionary for safe updating\n",
    "    \"\"\"\n",
    "    for key, val in dict(dict_from).iteritems():\n",
    "        if key in dict_to:\n",
    "            raise KeyError(key)\n",
    "        dict_to[key] = val\n",
    "    return dict_to\n",
    "    \n",
    "def get_idx_from_sent(sent, word_idx_map, max_l=51, k=300, filter_h=5):\n",
    "    \"\"\"\n",
    "    Transforms sentence into a list of indices. Pad with zeroes.\n",
    "    \"\"\"\n",
    "    x = []\n",
    "    pad = filter_h - 1\n",
    "    for i in xrange(pad):\n",
    "        x.append(0)\n",
    "    words = sent.split()\n",
    "    for word in words:\n",
    "        if word in word_idx_map:\n",
    "            x.append(word_idx_map[word])\n",
    "    while len(x) < max_l+2*pad:\n",
    "        x.append(0)\n",
    "    return x\n",
    "\n",
    "def make_idx_data_cv(revs, word_idx_map, cv, max_l=51, k=300, filter_h=5):\n",
    "    \"\"\"\n",
    "    Transforms sentences into a 2-d matrix.\n",
    "    \"\"\"\n",
    "    train, test = [], []\n",
    "    for rev in revs:\n",
    "        sent = get_idx_from_sent(rev[\"text\"], word_idx_map, max_l, k, filter_h)   \n",
    "        sent.append(rev[\"y\"])\n",
    "        if rev[\"split\"]==cv:            \n",
    "            test.append(sent)        \n",
    "        else:  \n",
    "            train.append(sent)   \n",
    "    train = np.array(train,dtype=\"int\")\n",
    "    test = np.array(test,dtype=\"int\")\n",
    "    return [train, test]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data... data loaded!\n",
      "model architecture: CNN-non-static\n",
      "using: word2vec vectors\n",
      "[('image shape', 64, 300), ('filter shape', [(100, 1, 3, 300), (100, 1, 4, 300), (100, 1, 5, 300)]), ('hidden_units', [100, 2]), ('dropout', [0.5]), ('batch_size', 50), ('non_static', True), ('learn_decay', 0.95), ('conv_non_linear', 'relu'), ('non_static', True), ('sqr_norm_lim', 9), ('shuffle_batch', True)]\n",
      "... training\n",
      "epoch 1, train perf 80.643678 %, val perf 79.157895\n",
      "epoch 2, train perf 87.241379 %, val perf 80.421053\n",
      "epoch 3, train perf 88.816092 %, val perf 78.421053\n",
      "epoch 4, train perf 94.241379 %, val perf 79.473684\n",
      "epoch 5, train perf 97.678161 %, val perf 82.105263\n",
      "epoch 6, train perf 99.034483 %, val perf 81.473684\n",
      "epoch 7, train perf 99.275862 %, val perf 81.894737\n",
      "epoch 8, train perf 99.781609 %, val perf 81.157895\n",
      "epoch 9, train perf 99.839080 %, val perf 82.631579\n",
      "epoch 10, train perf 99.919540 %, val perf 82.210526\n",
      "epoch 11, train perf 99.931034 %, val perf 82.315789\n",
      "epoch 12, train perf 99.988506 %, val perf 82.421053\n",
      "epoch 13, train perf 99.977011 %, val perf 82.105263\n",
      "epoch 14, train perf 99.988506 %, val perf 81.894737\n",
      "epoch 15, train perf 99.988506 %, val perf 82.421053\n",
      "epoch 16, train perf 99.988506 %, val perf 82.526316\n",
      "epoch 17, train perf 99.988506 %, val perf 82.631579\n",
      "epoch 18, train perf 99.988506 %, val perf 81.684211\n",
      "epoch 19, train perf 99.988506 %, val perf 82.315789\n",
      "epoch 20, train perf 99.988506 %, val perf 82.736842\n",
      "epoch 21, train perf 99.988506 %, val perf 82.421053\n",
      "epoch 22, train perf 99.988506 %, val perf 82.210526\n",
      "epoch 23, train perf 99.988506 %, val perf 82.421053\n",
      "epoch 24, train perf 100.000000 %, val perf 82.421053\n",
      "epoch 25, train perf 100.000000 %, val perf 82.000000\n",
      "cv: 0, perf: 0.816635160681\n",
      "[('image shape', 64, 300), ('filter shape', [(100, 1, 3, 300), (100, 1, 4, 300), (100, 1, 5, 300)]), ('hidden_units', [100, 2]), ('dropout', [0.5]), ('batch_size', 50), ('non_static', True), ('learn_decay', 0.95), ('conv_non_linear', 'relu'), ('non_static', True), ('sqr_norm_lim', 9), ('shuffle_batch', True)]\n",
      "... training\n",
      "epoch 1, train perf 81.283237 %, val perf 79.263158\n",
      "epoch 2, train perf 81.283237 %, val perf 76.315789\n",
      "epoch 3, train perf 90.231214 %, val perf 80.736842\n",
      "epoch 4, train perf 95.156069 %, val perf 82.421053\n",
      "epoch 5, train perf 98.161850 %, val perf 83.263158\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-738b08e92c9c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m                           \u001b[0mnon_static\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnon_static\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m                           \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m                           dropout_rate=[0.5])\n\u001b[0m\u001b[0;32m     36\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[1;34m\"cv: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\", perf: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mperf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mperf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-d5a34bb3aa05>\u001b[0m in \u001b[0;36mtrain_conv_net\u001b[1;34m(datasets, U, img_w, filter_hs, hidden_units, dropout_rate, shuffle_batch, n_epochs, batch_size, lr_decay, conv_non_linear, activations, sqr_norm_lim, non_static)\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mshuffle_batch\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mminibatch_index\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_train_batches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m                 \u001b[0mcost_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mminibatch_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    155\u001b[0m                 \u001b[0mset_zero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzero_vec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    593\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    594\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 595\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    596\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print \"loading data...\",\n",
    "x = cPickle.load(open(\"mr.p\",\"rb\"))\n",
    "revs, W, W2, word_idx_map, vocab = x[0], x[1], x[2], x[3], x[4]\n",
    "print \"data loaded!\"\n",
    "mode = \"-nonstatic\"\n",
    "word_vectors = \"-word2vec\"    \n",
    "if mode==\"-nonstatic\":\n",
    "    print \"model architecture: CNN-non-static\"\n",
    "    non_static=True\n",
    "elif mode==\"-static\":\n",
    "    print \"model architecture: CNN-static\"\n",
    "    non_static=False\n",
    "execfile(\"conv_net_classes.py\")    \n",
    "if word_vectors==\"-rand\":\n",
    "    print \"using: random vectors\"\n",
    "    U = W2\n",
    "elif word_vectors==\"-word2vec\":\n",
    "    print \"using: word2vec vectors\"\n",
    "    U = W\n",
    "results = []\n",
    "r = range(0,10)    \n",
    "for i in r:\n",
    "    datasets = make_idx_data_cv(revs, word_idx_map, i, max_l=56,k=300, filter_h=5)\n",
    "    perf = train_conv_net(datasets,\n",
    "                          U,\n",
    "                          lr_decay=0.95,\n",
    "                          filter_hs=[3,4,5],\n",
    "                          conv_non_linear=\"relu\",\n",
    "                          hidden_units=[100,2], \n",
    "                          shuffle_batch=True, \n",
    "                          n_epochs=25, \n",
    "                          sqr_norm_lim=9,\n",
    "                          non_static=non_static,\n",
    "                          batch_size=50,\n",
    "                          dropout_rate=[0.5])\n",
    "    print \"cv: \" + str(i) + \", perf: \" + str(perf)\n",
    "    results.append(perf)  \n",
    "print str(np.mean(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
