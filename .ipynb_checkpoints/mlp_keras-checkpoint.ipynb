{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils \n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import re\n",
    "from gensim.parsing import strip_multiple_whitespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### return sentence matrix\n",
    "def get_img(tokens,max_len = 50):\n",
    "    img = []\n",
    "    for w in tokens:\n",
    "        try:\n",
    "            img.append(model[w])\n",
    "        except:\n",
    "            pass\n",
    "    if len(img) >= max_len:\n",
    "        img = img[:max_len]\n",
    "    elif len(img) > 0:\n",
    "        n = max_len - len(img)\n",
    "        img = np.vstack([img,np.zeros((n,300))])\n",
    "    else:\n",
    "        img = np.zeros((max_len,300))\n",
    "    return np.array(img)\n",
    "\n",
    "def preprocess(sentence):\n",
    "    return strip_multiple_whitespaces(re.sub(r'[\\W_]+', ' ', sentence)).split(' ')\n",
    "def process_output(labels):\n",
    "    return np_utils.to_categorical(labels, nb_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_file =\"data/rt-polarity.pos\"\n",
    "neg_file = \"data/rt-polarity.neg\"\n",
    "w2v_file = \"data/GoogleNews-vectors-negative300.bin\"\n",
    "pos_sentences = open(pos_file).readlines()\n",
    "neg_sentences = open(neg_file).readlines()\n",
    "model = Word2Vec.load_word2vec_format(w2v_file,binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10662, 50, 300) (10662,)\n"
     ]
    }
   ],
   "source": [
    "pos_tokens = [preprocess(s) for s in pos_sentences]\n",
    "labels = [1]*len(pos_tokens)\n",
    "neg_tokens = [preprocess(s) for s in neg_sentences]\n",
    "labels += [0]*len(neg_sentences)\n",
    "imgs= [get_img(t) for t in pos_tokens+neg_tokens]\n",
    "imgs = np.array(imgs)\n",
    "labels = np.array(labels)\n",
    "print imgs.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8529, 15000) (8529, 2) (2133, 15000) (2133, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "train_X,test_X,train_y,test_y = train_test_split(imgs.reshape((imgs.shape[0],-1)),labels,test_size=0.2)\n",
    "train_y = process_output(train_y)\n",
    "test_y = process_output(test_y)\n",
    "print train_X.shape, train_y.shape, test_X.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5970 samples, validate on 2559 samples\n",
      "Epoch 0\n",
      "3s - loss: 0.6732 - acc: 0.5920 - val_loss: 0.6217 - val_acc: 0.6866\n",
      "Epoch 1\n",
      "3s - loss: 0.5668 - acc: 0.7228 - val_loss: 0.5625 - val_acc: 0.7112\n",
      "Epoch 2\n",
      "3s - loss: 0.4572 - acc: 0.7916 - val_loss: 0.5557 - val_acc: 0.7143\n",
      "Epoch 3\n",
      "3s - loss: 0.3716 - acc: 0.8363 - val_loss: 0.6593 - val_acc: 0.6971\n",
      "Epoch 4\n",
      "3s - loss: 0.2963 - acc: 0.8792 - val_loss: 0.7177 - val_acc: 0.6882\n",
      "Epoch 5\n",
      "3s - loss: 0.2330 - acc: 0.9060 - val_loss: 0.7314 - val_acc: 0.6928\n",
      "Epoch 6\n",
      "3s - loss: 0.1713 - acc: 0.9369 - val_loss: 0.7841 - val_acc: 0.7046\n",
      "Epoch 7\n",
      "3s - loss: 0.1280 - acc: 0.9541 - val_loss: 0.7727 - val_acc: 0.6975\n",
      "Epoch 8\n",
      "3s - loss: 0.1022 - acc: 0.9672 - val_loss: 0.8391 - val_acc: 0.7011\n",
      "Epoch 9\n",
      "3s - loss: 0.0414 - acc: 0.9911 - val_loss: 1.1068 - val_acc: 0.6948\n",
      "Epoch 10\n",
      "3s - loss: 0.0518 - acc: 0.9853 - val_loss: 1.1683 - val_acc: 0.7034\n",
      "Epoch 11\n",
      "3s - loss: 0.0165 - acc: 0.9978 - val_loss: 1.2881 - val_acc: 0.7100\n",
      "Epoch 12\n",
      "3s - loss: 0.0086 - acc: 0.9995 - val_loss: 1.3805 - val_acc: 0.7034\n",
      "Epoch 13\n",
      "3s - loss: 0.0087 - acc: 0.9992 - val_loss: 1.4647 - val_acc: 0.7003\n",
      "Epoch 14\n",
      "3s - loss: 0.0040 - acc: 0.9997 - val_loss: 1.5791 - val_acc: 0.6995\n",
      "Epoch 15\n",
      "3s - loss: 0.0022 - acc: 1.0000 - val_loss: 1.6654 - val_acc: 0.6960\n",
      "Epoch 16\n",
      "3s - loss: 0.0024 - acc: 0.9998 - val_loss: 1.6927 - val_acc: 0.7007\n",
      "Epoch 17\n",
      "3s - loss: 0.0022 - acc: 1.0000 - val_loss: 1.7358 - val_acc: 0.6968\n",
      "Epoch 18\n",
      "3s - loss: 0.0013 - acc: 1.0000 - val_loss: 1.7970 - val_acc: 0.6991\n",
      "Epoch 19\n",
      "3s - loss: 0.0010 - acc: 1.0000 - val_loss: 1.8258 - val_acc: 0.6999\n",
      "[1.7313131692551826, 0.70557899671823721]\n",
      "2133/2133 [==============================] - 0s     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.70557899671823721"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(50 * 300, 128, activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(128, 128, activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(128, 2, activation=\"softmax\"))\n",
    "rms = RMSprop()\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = rms)\n",
    "\n",
    "## train model under an sklearn interface\n",
    "## model snapshot callback\n",
    "#save_model = ModelCheckpoint(\"data/tmp/keras_mlp.h5\")\n",
    "model.fit(train_X, train_y, batch_size=100, nb_epoch=20, \n",
    "          show_accuracy=True, verbose=2, validation_split=0.3)\n",
    "\n",
    "## evaluate on test data\n",
    "print model.evaluate(test_X, test_y, show_accuracy=True, verbose=0)\n",
    "np.mean(model.predict_classes(test_X) == test_y.argmax(axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
